============
Introduction
============

The constant improvement of astronomical instrumentation provides the
foundation for scientific discoveries. In general, these improvements
have only implications forward in time, while previous observations do
not profit from this trend. In solar physics, the study of long-term evolution
typically exceeds the lifetime of single instruments and data driven approaches
are strongly limited in terms of coherent long-term data samples.
We demonstrate that the available data sets can directly profit from the most
recent instrumental improvements and provide a so far unused resource to foster
novel research and accelerate data driven studies.
Here we provide a general method that translates between image domains of different
instruments (Instrument-to-Instrument translation; ITI), in order to inter-calibrate
data sets, enhance physically relevant features which are otherwise beyond the diffraction
limit of the telescope, mitigate atmospheric degradation effects and can estimate observables
that are not covered by the instrument.
We demonstrate that our method can provide unified long-term data sets at the highest quality,
by applying it to five different applications of ground- and space-based solar observations.
We obtain 1) a homogeneous data series of 24 years of space-based observations of the solar corona,
2) solar full-disk observations with unprecedented spatial resolution,
3) real-time mitigation of atmospheric degradations in ground-based observations,
4) a uniform series of ground-based H-alpha observations starting from 1973, that unifies solar observations recorded on photographic film and CCD,
5) magnetic field estimates from the solar far-side based on multi-band EUV imagery.
The direct comparison to simultaneous high-quality observations shows that our method produces images that are perceptually similar and match the reference image distribution.

Instrument-to-Instrument translation is designed as general framework that can be easily applied to similar tasks. Many of the basic data loading, normalizing and scaling operations are already implemented by editors and can be used for the creation of new data sets, while also new custom editors can be added.